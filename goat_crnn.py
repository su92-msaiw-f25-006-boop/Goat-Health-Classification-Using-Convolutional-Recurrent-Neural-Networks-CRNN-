# -*- coding: utf-8 -*-
"""GOAT_CRNN.ipynb

Veterinary Disease Classification in Goats Using CRNNs
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BZiKy1l4gTWpGj1JRMkONzhHYfXB4jT_
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Dense, Dropout,
    BatchNormalization, Input, GRU, Reshape
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Dataset directory configuration
BASE_DIR = "/content/drive/MyDrive/Goat_Splits"

# Define dataset split directories
TRAIN_DIR = os.path.join(BASE_DIR, "train")
VAL_DIR   = os.path.join(BASE_DIR, "val")
TEST_DIR  = os.path.join(BASE_DIR, "test")

# Hyperparameters configuration
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 30
LEARNING_RATE = 1e-4

# Load training dataset
train_ds = image_dataset_from_directory(
    TRAIN_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    seed=42
)

# Load validation dataset
val_ds = image_dataset_from_directory(
    VAL_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    seed=42
)

# Load test dataset (no shuffling for consistent evaluation)
test_ds = image_dataset_from_directory(
    TEST_DIR,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=False
)

# Extract class names and count
class_names = train_ds.class_names
NUM_CLASSES = len(class_names)

# Display dataset information
print("Classes:", class_names)
print("Number of Classes:", NUM_CLASSES)

# Data augmentation pipeline for training set
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomZoom(0.1),
    tf.keras.layers.RandomContrast(0.1)
])

# Apply data augmentation to training set
train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
# Normalize validation and test sets
val_ds   = val_ds.map(lambda x, y: (x / 255.0, y))
test_ds  = test_ds.map(lambda x, y: (x / 255.0, y))

# Optimize data pipeline with prefetching
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds  = test_ds.prefetch(tf.data.AUTOTUNE)

# Define model input
inputs = Input(shape=(128, 128, 3))

# ----- CNN FEATURE EXTRACTOR -----
x = Conv2D(16, 3, activation='relu', padding='same')(inputs)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(32, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(64, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(128, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

# ----- CNN ‚Üí SEQUENCE CONVERSION -----
# Reshape feature maps to sequences for RNN processing
x = Reshape((8, 8 * 128))(x)   # (timesteps, features)

# ----- RNN LAYER -----
# GRU layer for sequential pattern recognition
x = GRU(128, return_sequences=False)(x)

# ----- CLASSIFIER -----
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs, outputs)

model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

model.summary()

early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=7,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    "best_goat_crnn_model.keras",
    monitor='val_accuracy',
    save_best_only=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[early_stop, checkpoint, reduce_lr]
)

MODEL_PATH = "/content/drive/MyDrive/Goat_CRNN_Model.keras"
model.save(MODEL_PATH)
print("‚úÖ Model saved successfully")

test_loss, test_acc = model.evaluate(test_ds)
print(f"üî• Test Accuracy: {test_acc * 100:.2f}%")

y_true, y_pred = [], []

for x, y in test_ds:
    preds = model.predict(x)
    y_true.extend(np.argmax(y.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

cm = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(7,5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

IMAGE_PATH = "/content/drive/MyDrive/test_image/goat_test1.jpg"

model = tf.keras.models.load_model(MODEL_PATH)

img = tf.keras.preprocessing.image.load_img(
    IMAGE_PATH, target_size=IMG_SIZE
)

img_arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0
img_arr = np.expand_dims(img_arr, axis=0)

pred = model.predict(img_arr)
idx = np.argmax(pred[0])

predicted_class = class_names[idx]
confidence = np.max(pred[0]) * 100

status = "HEALTHY ‚úÖ" if "healthy" in predicted_class.lower() else "DISEASED ‚ùå"

plt.imshow(img)
plt.axis("off")
plt.title(f"{predicted_class}\n{status} ({confidence:.2f}%)")
plt.show()

print("Predicted Class :", predicted_class)
print("Status          :", status)
print(f"Confidence      : {confidence:.2f}%")